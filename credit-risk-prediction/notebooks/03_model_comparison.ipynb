{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Credit Risk Prediction - Model Comparison\n",
                "\n",
                "This notebook compares different machine learning models for credit risk prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.metrics import roc_curve, auc, precision_recall_curve, classification_report\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "df = pd.read_csv('../data/features/credit_features.csv')\n",
                "print(f'Dataset Shape: {df.shape}')\n",
                "\n",
                "# Prepare features\n",
                "exclude_cols = ['customer_id', 'default']\n",
                "feature_cols = [col for col in df.columns if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]\n",
                "\n",
                "X = df[feature_cols].fillna(0).values\n",
                "y = df['default'].values\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "print(f'Training set: {len(X_train)}, Test set: {len(X_test)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Train Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale features for Logistic Regression\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "# Define models\n",
                "models = {\n",
                "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
                "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42),\n",
                "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
                "}\n",
                "\n",
                "# Train and evaluate\n",
                "results = {}\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f'Training {name}...')\n",
                "    \n",
                "    # Use scaled data for LR, original for tree-based\n",
                "    if 'Logistic' in name:\n",
                "        model.fit(X_train_scaled, y_train)\n",
                "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
                "    else:\n",
                "        model.fit(X_train, y_train)\n",
                "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    # Calculate metrics\n",
                "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
                "    roc_auc = auc(fpr, tpr)\n",
                "    \n",
                "    results[name] = {\n",
                "        'model': model,\n",
                "        'y_pred_proba': y_pred_proba,\n",
                "        'fpr': fpr,\n",
                "        'tpr': tpr,\n",
                "        'auc': roc_auc\n",
                "    }\n",
                "    \n",
                "    print(f'  AUC: {roc_auc:.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ROC Curve Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "\n",
                "colors = ['blue', 'green', 'red', 'purple']\n",
                "for (name, result), color in zip(results.items(), colors):\n",
                "    plt.plot(result['fpr'], result['tpr'], color=color, lw=2,\n",
                "             label=f\"{name} (AUC = {result['auc']:.3f})\")\n",
                "\n",
                "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
                "plt.xlim([0.0, 1.0])\n",
                "plt.ylim([0.0, 1.05])\n",
                "plt.xlabel('False Positive Rate', fontsize=12)\n",
                "plt.ylabel('True Positive Rate', fontsize=12)\n",
                "plt.title('ROC Curve Comparison', fontsize=14)\n",
                "plt.legend(loc='lower right', fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Comparison Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison table\n",
                "comparison_df = pd.DataFrame({\n",
                "    'Model': list(results.keys()),\n",
                "    'AUC-ROC': [results[m]['auc'] for m in results],\n",
                "    'Gini': [2*results[m]['auc'] - 1 for m in results]\n",
                "}).sort_values('AUC-ROC', ascending=False)\n",
                "\n",
                "print('Model Comparison:')\n",
                "print('='*50)\n",
                "comparison_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Feature Importance (Best Model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get best model (Random Forest or Gradient Boosting)\n",
                "best_model_name = comparison_df.iloc[0]['Model']\n",
                "best_model = results[best_model_name]['model']\n",
                "\n",
                "print(f'Best Model: {best_model_name}')\n",
                "\n",
                "if hasattr(best_model, 'feature_importances_'):\n",
                "    importance = best_model.feature_importances_\n",
                "    importance_df = pd.DataFrame({\n",
                "        'feature': feature_cols,\n",
                "        'importance': importance\n",
                "    }).sort_values('importance', ascending=False)\n",
                "    \n",
                "    plt.figure(figsize=(10, 8))\n",
                "    top_15 = importance_df.head(15)\n",
                "    plt.barh(top_15['feature'], top_15['importance'], color='steelblue')\n",
                "    plt.xlabel('Importance')\n",
                "    plt.title(f'Top 15 Feature Importance ({best_model_name})', fontsize=14)\n",
                "    plt.gca().invert_yaxis()\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Conclusion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Model Comparison Complete!')\n",
                "print('='*50)\n",
                "print(f'Best Performing Model: {best_model_name}')\n",
                "print(f'AUC-ROC: {comparison_df.iloc[0][\"AUC-ROC\"]:.4f}')\n",
                "print(f'Gini Coefficient: {comparison_df.iloc[0][\"Gini\"]:.4f}')\n",
                "print('\\nRecommendation: Use the best model for production deployment.')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
